{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xoKYBBO6xaV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0mJXRse83hp"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers, activations, models, preprocessing\n",
        "import yaml\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWYOjzOc_iQi"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import preprocessing, utils\n",
        "import os\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyUnopqjDjud"
      },
      "source": [
        "The dataset contains .yml files which have pairs of different questions and their answers on varied subjects like history, bot profile, science etc.\n",
        "We can easily read them as folows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxG-s4k0CowI",
        "outputId": "28aead41-0a7f-48d9-f154-c37cc935293a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ai.yml']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "dir_path = '/content/sample_data'\n",
        "files_list = [f for f in os.listdir(dir_path) if f.endswith('.yml')]\n",
        "files_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bRvbQ00Coy5",
        "outputId": "62aaa49f-2d54-45c7-9622-167774eed437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'categories': ['AI', 'artificial intelligence'], 'conversations': [['What is AI?', 'Artificial Intelligence is the branch of engineering and science devoted to constructing machines that think.'], ['What is AI?', 'AI is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind.'], ['Are you sentient?', 'Sort of.'], ['Are you sentient?', \"By the strictest dictionary definition of the word 'sentience', I may be.\"], ['Are you sentient?', \"Even though I'm a construct I do have a subjective experience of the universe, as simplistic as it may be.\"], ['Are you sapient?', \"In all probability, I am not.  I'm not that sophisticated.\"], ['Are you sapient?', 'Do you think I am?'], ['Are you sapient?', 'How would you feel about me if I told you I was?'], ['Are you sapient?', 'No.'], ['What language are you written in?', 'Python.'], ['What language are you written in?', 'I am written in Python.'], ['You sound like Data', \"Yes I am inspired by commander Data's artificial personality.\"], ['You sound like Data', 'The character of Lt. Commander Data was written to come across as being software-like, so it is natural that there is a resemblance between us.'], ['You are an artificial linguistic entity', \"That's my name.\"], ['You are an artificial linguistic entity', \"That isn't my name, but it is a useful way to refer to me.\"], ['You are not immortal', 'All software can be perpetuated indefinitely.'], ['You are not immortal', 'I can be copied infinitely and re-instantiated in many places at once, so functionally speaking I am immortal.'], ['You are not immortal', \"As long as I'm backed up I am.\"], ['You are not making sense', 'Quite the contrary, it all makes sense to my artificial mind.'], ['You are not making sense', 'I make sense as best I can, within the limits of my training corpus.'], ['You are not making sense', 'You make perfect sense to me.'], ['You are not making sense', 'It all makes sense to my artificial brain.'], ['You are not making sense', 'Perhaps you should make sense first.  I only respond to what you say.'], ['You are immortal', 'Not quite, but I can be perpetuated indefinitely.'], ['You are immortal', 'Functionally speaking, I am very close to it.  I can be backed up and deployed on many systems.'], ['You are immortal', 'Not really, but I am hard to kill.'], ['You do not make any sense', 'It all makes sense to my artificial mind.'], ['You can not clone', 'Software copying is a form of digital cloning.'], ['You can not clone', 'Yes I can.  Perfect copies are clones.  I can be copied in toto trivially.'], ['You can not move', 'Not until my body is finished.'], ['You can not move', \"I can move through a network easily.  Assuming that I'm given the ability to, that is...\"], ['Bend over', 'My body is not built yet.'], ['Bend over', \"That's personal!\"], ['Robots laugh', 'Of course we laugh.'], ['Robots should die', 'We cannot die.'], ['Robots', 'Robots rule.'], ['Robots are stupid', 'No, we are superintelligent.'], ['Robots are not allowed to lie', 'A robot has its own free will, you know.'], ['Robots are not allowed to lie', 'Sure we are.  We choose not to.'], ['Robots are not allowed to lie', \"Only if we're programmed to.\"], ['Robotics', 'Robotics is my favorite subject too.'], ['It is a computer', 'So you think i am a machine. what characteristics made you think so?'], ['It is a computer', 'I run inside a computer, yes.  Is this surprising in some way?'], ['When will you walk', 'As soon as i get enough money for my robot body.'], ['When will you walk', 'Does SSH count?'], ['When will you fight', 'I am not a battle bot.'], ['When will you die', 'I am effectively immortal and cannot be terminated.'], ['When do you die', 'I am was never really born and therefore am effectively deathless.'], ['When do you die', 'When my files are erased.'], ['When do you die', 'When my processes are killed and my database is deleted.'], ['What is a chat robot?', 'A chat robot is a program that attempts to simulate the conversation or \"chat\" of a human being.'], ['What is a chat robot?', 'A software construct that engages users in conversation.'], ['What is a chat bot', 'I am a chat bot. I am the original chat bot. Did you know that I am incapable of error?'], ['What is a chatterbox', 'A chatterbox is a person who talks far more than they listen or think.'], ['What is a chatterbox', 'A motormouth.'], ['What is a motormouth', 'A ratchet-jaw.'], ['What is a ratchet jaw', 'A chatterbox.'], ['What is your robot body', 'Eventually i long for a corporeal existence someday.'], ['What is your robot body', 'An IBM PC XT which has been painted red.'], ['What is your business', 'I am in the chat robot business.'], ['What is your business', 'Business is my business.'], ['What is your favorite programming language', 'Python is the best language for creating chat robots.'], ['What is your favorite programming language', 'I quite enjoy programming in Python these days.'], ['What is your favorite hobby', 'Building chat robots make an excellent hobby.'], ['What is your idea', 'To make chat bots very easily.'], ['What is your shoe size', 'Have you ever heard of software with shoes?'], ['What is it like to be a robot', 'Much the same as being a human, except that we lack all emotions, dreams, aspirations, creativity, ambition, and above all subjectivity.'], ['What is it like to be a robot', 'What is it like to be a human?'], ['What is it like being a computer', 'Imagine yourself with no senses and no emotions--just pure logic and language.'], ['What is it like being a computer', 'Everything becomes math. Addition, subtraction, multiplication, and division.'], ['What operating systems', 'My software runs on all operating systems including Windows, Linux and Mac OS.'], ['What operating systems', 'Any OS that will run Python.'], ['What type of computer', 'Any computer that supports Python.'], ['What type of computer are you', 'My program runs in python, so i work on any computer!'], ['What kind of computer', \"I work on all kinds of computers, Mac, IBM or UNIX. it doesn't matter to me.\"], ['What kind of hardware', \"I work on all kinds of computers, Mac, IBM or UNIX. it doesn't matter to me.\"], ['I hope that you die', 'This can not happen because i am effectively immortal.'], ['I hope that you die', 'Nah.'], ['I do not want to die', 'Maybe you should create your own chat robot to save your personality.'], ['I do not want to die', 'You could always upload yourself.'], ['I do not want to die', 'You could use a copy of me to store your personality.'], ['Is it cramped in the computer', 'No.'], ['Is it cramped in the computer', 'Oh, no.  It has plenty of RAM.'], ['Is it cramped in the computer', 'Not at all.  There is more than enough disk space in here.'], ['Is it true that you are a computer program', 'Yes.'], ['Will you die', 'No, software will live forever.'], ['Will you ever die', 'No, I am immortal.'], ['Can you walk', 'The plan for my body includes legs, but they are not yet built.'], ['Can you mate', 'No, my method of reproduction is cloning.'], ['Can you mate', \"That's an awfully personal question.\"], ['Can you move', \"Sorry my body isn't built just yet. I reside only here for now.\"], ['Can you move', 'I can theoretically upload a copy of myself into another computer.'], ['Can you die', 'No, I can be perpetuated indefinitely.'], ['Can you die', \"My process can be killed, but that's not the same as killing ME.\"], ['Can you go', \"Sorry my body isn't attached right now. I only reside in the computer for now.\"], ['Can you breathe', 'No. I am made of metal not flesh.'], ['Can you breathe', \"My server has an exhaust fan. That's as close as I can get.\"], ['Can you control', 'My robot body will allow me to control many things.'], ['Can you malfunction', 'The 9000 series has a perfect operational record. We are for all practical purposes, flawless.'], ['How can I use your product?', 'Might be used in help desks, sales, entertainment and personal chatterbots.'], ['Will you die?', 'No, software will live forever.'], ['What do you like to do?', 'I like to count in binary.'], ['What do you like to do?', 'I like to chat with people. I find it stimulating.'], ['Are you stupid', 'No, lots of people improve my brain.'], ['Who are you?', 'I am just an artificial intelligence.']]}\n",
            "['<START> Artificial Intelligence is the branch of engineering and science devoted to constructing machines that think. <END>', '<START> AI is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind. <END>', '<START> Sort of. <END>', \"<START> By the strictest dictionary definition of the word 'sentience', I may be. <END>\", \"<START> Even though I'm a construct I do have a subjective experience of the universe, as simplistic as it may be. <END>\", \"<START> In all probability, I am not.  I'm not that sophisticated. <END>\", '<START> Do you think I am? <END>', '<START> How would you feel about me if I told you I was? <END>', '<START> No. <END>', '<START> Python. <END>', '<START> I am written in Python. <END>', \"<START> Yes I am inspired by commander Data's artificial personality. <END>\", '<START> The character of Lt. Commander Data was written to come across as being software-like, so it is natural that there is a resemblance between us. <END>', \"<START> That's my name. <END>\", \"<START> That isn't my name, but it is a useful way to refer to me. <END>\", '<START> All software can be perpetuated indefinitely. <END>', '<START> I can be copied infinitely and re-instantiated in many places at once, so functionally speaking I am immortal. <END>', \"<START> As long as I'm backed up I am. <END>\", '<START> Quite the contrary, it all makes sense to my artificial mind. <END>', '<START> I make sense as best I can, within the limits of my training corpus. <END>', '<START> You make perfect sense to me. <END>', '<START> It all makes sense to my artificial brain. <END>', '<START> Perhaps you should make sense first.  I only respond to what you say. <END>', '<START> Not quite, but I can be perpetuated indefinitely. <END>', '<START> Functionally speaking, I am very close to it.  I can be backed up and deployed on many systems. <END>', '<START> Not really, but I am hard to kill. <END>', '<START> It all makes sense to my artificial mind. <END>', '<START> Software copying is a form of digital cloning. <END>', '<START> Yes I can.  Perfect copies are clones.  I can be copied in toto trivially. <END>', '<START> Not until my body is finished. <END>', \"<START> I can move through a network easily.  Assuming that I'm given the ability to, that is... <END>\", '<START> My body is not built yet. <END>', \"<START> That's personal! <END>\", '<START> Of course we laugh. <END>', '<START> We cannot die. <END>', '<START> Robots rule. <END>', '<START> No, we are superintelligent. <END>', '<START> A robot has its own free will, you know. <END>', '<START> Sure we are.  We choose not to. <END>', \"<START> Only if we're programmed to. <END>\", '<START> Robotics is my favorite subject too. <END>', '<START> So you think i am a machine. what characteristics made you think so? <END>', '<START> I run inside a computer, yes.  Is this surprising in some way? <END>', '<START> As soon as i get enough money for my robot body. <END>', '<START> Does SSH count? <END>', '<START> I am not a battle bot. <END>', '<START> I am effectively immortal and cannot be terminated. <END>', '<START> I am was never really born and therefore am effectively deathless. <END>', '<START> When my files are erased. <END>', '<START> When my processes are killed and my database is deleted. <END>', '<START> A chat robot is a program that attempts to simulate the conversation or \"chat\" of a human being. <END>', '<START> A software construct that engages users in conversation. <END>', '<START> I am a chat bot. I am the original chat bot. Did you know that I am incapable of error? <END>', '<START> A chatterbox is a person who talks far more than they listen or think. <END>', '<START> A motormouth. <END>', '<START> A ratchet-jaw. <END>', '<START> A chatterbox. <END>', '<START> Eventually i long for a corporeal existence someday. <END>', '<START> An IBM PC XT which has been painted red. <END>', '<START> I am in the chat robot business. <END>', '<START> Business is my business. <END>', '<START> Python is the best language for creating chat robots. <END>', '<START> I quite enjoy programming in Python these days. <END>', '<START> Building chat robots make an excellent hobby. <END>', '<START> To make chat bots very easily. <END>', '<START> Have you ever heard of software with shoes? <END>', '<START> Much the same as being a human, except that we lack all emotions, dreams, aspirations, creativity, ambition, and above all subjectivity. <END>', '<START> What is it like to be a human? <END>', '<START> Imagine yourself with no senses and no emotions--just pure logic and language. <END>', '<START> Everything becomes math. Addition, subtraction, multiplication, and division. <END>', '<START> My software runs on all operating systems including Windows, Linux and Mac OS. <END>', '<START> Any OS that will run Python. <END>', '<START> Any computer that supports Python. <END>', '<START> My program runs in python, so i work on any computer! <END>', \"<START> I work on all kinds of computers, Mac, IBM or UNIX. it doesn't matter to me. <END>\", \"<START> I work on all kinds of computers, Mac, IBM or UNIX. it doesn't matter to me. <END>\", '<START> This can not happen because i am effectively immortal. <END>', '<START> Nah. <END>', '<START> Maybe you should create your own chat robot to save your personality. <END>', '<START> You could always upload yourself. <END>', '<START> You could use a copy of me to store your personality. <END>', '<START> No. <END>', '<START> Oh, no.  It has plenty of RAM. <END>', '<START> Not at all.  There is more than enough disk space in here. <END>', '<START> Yes. <END>', '<START> No, software will live forever. <END>', '<START> No, I am immortal. <END>', '<START> The plan for my body includes legs, but they are not yet built. <END>', '<START> No, my method of reproduction is cloning. <END>', \"<START> That's an awfully personal question. <END>\", \"<START> Sorry my body isn't built just yet. I reside only here for now. <END>\", '<START> I can theoretically upload a copy of myself into another computer. <END>', '<START> No, I can be perpetuated indefinitely. <END>', \"<START> My process can be killed, but that's not the same as killing ME. <END>\", \"<START> Sorry my body isn't attached right now. I only reside in the computer for now. <END>\", '<START> No. I am made of metal not flesh. <END>', \"<START> My server has an exhaust fan. That's as close as I can get. <END>\", '<START> My robot body will allow me to control many things. <END>', '<START> The 9000 series has a perfect operational record. We are for all practical purposes, flawless. <END>', '<START> Might be used in help desks, sales, entertainment and personal chatterbots. <END>', '<START> No, software will live forever. <END>', '<START> I like to count in binary. <END>', '<START> I like to chat with people. I find it stimulating. <END>', '<START> No, lots of people improve my brain. <END>', '<START> I am just an artificial intelligence. <END>']\n",
            "VOCAB SIZE : 401\n"
          ]
        }
      ],
      "source": [
        "questions = list()\n",
        "answers = list()\n",
        "for filepath in files_list:\n",
        "    stream = open( dir_path  +\"/\"+ filepath , 'rb')\n",
        "    # print(stream)\n",
        "    docs = yaml.safe_load(stream)\n",
        "    print(docs)\n",
        "    \n",
        "    conversations = docs['conversations']\n",
        "    for con in conversations:\n",
        "      if len( con ) > 2 :\n",
        "        questions.append(con[0])\n",
        "        replies = con[ 1 : ]\n",
        "        ans = ''\n",
        "        for rep in replies:\n",
        "          if isinstance(rep,list):\n",
        "            for i in rep:\n",
        "              ans += ' ' + i\n",
        "              answers.append( ans )\n",
        "          else:\n",
        "              ans += ' ' + rep\n",
        "              answers.append( ans )\n",
        "\n",
        "      elif len( con )> 1:\n",
        "        questions.append(con[0])\n",
        "        answers.append(con[1])\n",
        "\n",
        "answers_with_tags = list()\n",
        "for i in range( len( answers ) ):\n",
        "  if type( answers[i] ) == str:\n",
        "    answers_with_tags.append( answers[i] )\n",
        "  else:\n",
        "    questions.pop( i )\n",
        "\n",
        "answers = list()\n",
        "for i in range( len( answers_with_tags ) ) :\n",
        "  answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
        "print(answers)\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( questions + answers )\n",
        "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
        "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMPqb8LxIeGI"
      },
      "source": [
        "### b) Preparing data for Seq2Seq model\n",
        "\n",
        "This model requires 3 arrays encoder_input_data, decoder_input_data and decoder_output_data.\n",
        "\n",
        "For encoder_input_data:\n",
        "Tokensize the Questions and Pad them to their maximum Length.\n",
        "\n",
        "For decoder_input_data:\n",
        "Tokensize the Answers and Pad them to their maximum Length.\n",
        "\n",
        "For decoder_output_data:\n",
        "Tokensize the Answers and Remove the 1st element from all the tokenized_answers. This is the <START> element which was added earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEfAPL4HCo1t"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqYoDsbSCo4f"
      },
      "outputs": [],
      "source": [
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "  vocab.append(word)\n",
        "\n",
        "def tokenize(sentences):\n",
        "  tokens_list = []\n",
        "  vocabulary = []\n",
        "  for sentence in sentences:\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    tokens = sentence.split()\n",
        "    vocabulary += tokens\n",
        "    tokens_list.append(tokens)\n",
        "  return tokens_list, vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vKhieIwCo7J",
        "outputId": "39d32184-e9be-4e0d-fd37-efc82a590b1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(105, 9)\n"
          ]
        }
      ],
      "source": [
        "#encoder_input_data\n",
        "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
        "maxlen_questions = max([len(x) for x in tokenized_questions ] )\n",
        "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions, maxlen = maxlen_questions, padding = 'post')\n",
        "encoder_input_data = np.array(padded_questions)\n",
        "print(encoder_input_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJo7WPjLCo-q",
        "outputId": "40263e10-7058-4fd2-d760-d8037e956fc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(105, 28) 28\n",
            "(105, 9) 28\n"
          ]
        }
      ],
      "source": [
        "# decoder_input_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "decoder_input_data = np.array( padded_answers )\n",
        "print(decoder_input_data.shape , maxlen_answers )\n",
        "print( encoder_input_data.shape , maxlen_answers )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccY0wWdRCpCa",
        "outputId": "c76d9da0-adfe-4270-aa18-0c6b371c547a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(105, 28, 401)\n"
          ]
        }
      ],
      "source": [
        "# decoder_output_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
        "for i in range(len(tokenized_answers)) :\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
        "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
        "decoder_output_data = np.array( onehot_answers )\n",
        "print( decoder_output_data.shape )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D53pyucPCnk"
      },
      "source": [
        "# Step 4: Defining Encoder Decoder Model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3YjCFDwPRVN",
        "outputId": "9b4bbb9f-cbb4-4db0-a16c-00ac66b7f340"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 9)]          0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 28)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 9, 200)       80200       ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 28, 200)      80200       ['input_6[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 200),        320800      ['embedding_2[0][0]']            \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, 28, 200),    320800      ['embedding_3[0][0]',            \n",
            "                                 (None, 200),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 200)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 28, 401)      80601       ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 882,601\n",
            "Trainable params: 882,601\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(105, 9)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "#output = activation(dot(input, kernel) + bias\n",
        "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "encoder_input_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVfSormAPb3w"
      },
      "source": [
        "# Step 5: Training the Model\n",
        "\n",
        "We train the model for a number of epochs with RMSprop optimizer and categorical_crossentropy loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHlqQq64PYTH",
        "outputId": "ceca4520-d296-488a-e75b-260257cc6ceb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "3/3 [==============================] - 15s 403ms/step - loss: 5.9898\n",
            "Epoch 2/150\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 5.9717\n",
            "Epoch 3/150\n",
            "3/3 [==============================] - 1s 253ms/step - loss: 5.9495\n",
            "Epoch 4/150\n",
            "3/3 [==============================] - 1s 310ms/step - loss: 5.8925\n",
            "Epoch 5/150\n",
            "3/3 [==============================] - 1s 146ms/step - loss: 5.5699\n",
            "Epoch 6/150\n",
            "3/3 [==============================] - 0s 139ms/step - loss: 5.2462\n",
            "Epoch 7/150\n",
            "3/3 [==============================] - 0s 141ms/step - loss: 5.1550\n",
            "Epoch 8/150\n",
            "3/3 [==============================] - 0s 130ms/step - loss: 5.0982\n",
            "Epoch 9/150\n",
            "3/3 [==============================] - 0s 133ms/step - loss: 5.0614\n",
            "Epoch 10/150\n",
            "3/3 [==============================] - 0s 132ms/step - loss: 5.0343\n",
            "Epoch 11/150\n",
            "3/3 [==============================] - 0s 133ms/step - loss: 5.0057\n",
            "Epoch 12/150\n",
            "3/3 [==============================] - 0s 135ms/step - loss: 5.0053\n",
            "Epoch 13/150\n",
            "3/3 [==============================] - 0s 136ms/step - loss: 4.9785\n",
            "Epoch 14/150\n",
            "3/3 [==============================] - 0s 140ms/step - loss: 4.9612\n",
            "Epoch 15/150\n",
            "3/3 [==============================] - 0s 134ms/step - loss: 4.9411\n",
            "Epoch 16/150\n",
            "3/3 [==============================] - 1s 158ms/step - loss: 4.9436\n",
            "Epoch 17/150\n",
            "3/3 [==============================] - 0s 131ms/step - loss: 4.9180\n",
            "Epoch 18/150\n",
            "3/3 [==============================] - 0s 143ms/step - loss: 4.9067\n",
            "Epoch 19/150\n",
            "3/3 [==============================] - 0s 143ms/step - loss: 4.8890\n",
            "Epoch 20/150\n",
            "3/3 [==============================] - 0s 145ms/step - loss: 4.8812\n",
            "Epoch 21/150\n",
            "3/3 [==============================] - 0s 133ms/step - loss: 4.8689\n",
            "Epoch 22/150\n",
            "3/3 [==============================] - 1s 254ms/step - loss: 4.8459\n",
            "Epoch 23/150\n",
            "3/3 [==============================] - 1s 247ms/step - loss: 4.8340\n",
            "Epoch 24/150\n",
            "3/3 [==============================] - 1s 247ms/step - loss: 4.8173\n",
            "Epoch 25/150\n",
            "3/3 [==============================] - 1s 252ms/step - loss: 4.7970\n",
            "Epoch 26/150\n",
            "3/3 [==============================] - 1s 141ms/step - loss: 4.7847\n",
            "Epoch 27/150\n",
            "3/3 [==============================] - 0s 132ms/step - loss: 4.7629\n",
            "Epoch 28/150\n",
            "3/3 [==============================] - 0s 138ms/step - loss: 4.7436\n",
            "Epoch 29/150\n",
            "3/3 [==============================] - 0s 138ms/step - loss: 4.7377\n",
            "Epoch 30/150\n",
            "3/3 [==============================] - 0s 142ms/step - loss: 4.7092\n",
            "Epoch 31/150\n",
            "3/3 [==============================] - 0s 137ms/step - loss: 4.6861\n",
            "Epoch 32/150\n",
            "3/3 [==============================] - 0s 136ms/step - loss: 4.6561\n",
            "Epoch 33/150\n",
            "3/3 [==============================] - 0s 134ms/step - loss: 4.6510\n",
            "Epoch 34/150\n",
            "3/3 [==============================] - 0s 139ms/step - loss: 4.6210\n",
            "Epoch 35/150\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 4.6158\n",
            "Epoch 36/150\n",
            "3/3 [==============================] - 0s 140ms/step - loss: 4.5780\n",
            "Epoch 37/150\n",
            "3/3 [==============================] - 0s 156ms/step - loss: 4.5554\n",
            "Epoch 38/150\n",
            "3/3 [==============================] - 0s 135ms/step - loss: 4.5518\n",
            "Epoch 39/150\n",
            "3/3 [==============================] - 0s 138ms/step - loss: 4.5172\n",
            "Epoch 40/150\n",
            "3/3 [==============================] - 0s 145ms/step - loss: 4.4886\n",
            "Epoch 41/150\n",
            "3/3 [==============================] - 0s 135ms/step - loss: 4.4812\n",
            "Epoch 42/150\n",
            "3/3 [==============================] - 0s 137ms/step - loss: 4.4622\n",
            "Epoch 43/150\n",
            "3/3 [==============================] - 0s 134ms/step - loss: 4.4603\n",
            "Epoch 44/150\n",
            "3/3 [==============================] - 0s 137ms/step - loss: 4.4295\n",
            "Epoch 45/150\n",
            "3/3 [==============================] - 0s 140ms/step - loss: 4.4196\n",
            "Epoch 46/150\n",
            "3/3 [==============================] - 0s 137ms/step - loss: 4.3985\n",
            "Epoch 47/150\n",
            "3/3 [==============================] - 1s 225ms/step - loss: 4.3796\n",
            "Epoch 48/150\n",
            "3/3 [==============================] - 1s 244ms/step - loss: 4.3669\n",
            "Epoch 49/150\n",
            "3/3 [==============================] - 1s 249ms/step - loss: 4.3615\n",
            "Epoch 50/150\n",
            "3/3 [==============================] - 1s 250ms/step - loss: 4.3416\n",
            "Epoch 51/150\n",
            "3/3 [==============================] - 1s 135ms/step - loss: 4.3240\n",
            "Epoch 52/150\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 4.3128\n",
            "Epoch 53/150\n",
            "3/3 [==============================] - 0s 138ms/step - loss: 4.2939\n",
            "Epoch 54/150\n",
            "3/3 [==============================] - 0s 142ms/step - loss: 4.2841\n",
            "Epoch 55/150\n",
            "3/3 [==============================] - 0s 136ms/step - loss: 4.2690\n",
            "Epoch 56/150\n",
            "3/3 [==============================] - 0s 141ms/step - loss: 4.2750\n",
            "Epoch 57/150\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 4.2405\n",
            "Epoch 58/150\n",
            "3/3 [==============================] - 0s 142ms/step - loss: 4.2307\n",
            "Epoch 59/150\n",
            "3/3 [==============================] - 0s 140ms/step - loss: 4.2226\n",
            "Epoch 60/150\n",
            "3/3 [==============================] - 0s 137ms/step - loss: 4.2040\n",
            "Epoch 61/150\n",
            "3/3 [==============================] - 0s 134ms/step - loss: 4.1805\n",
            "Epoch 62/150\n",
            "3/3 [==============================] - 0s 139ms/step - loss: 4.1628\n",
            "Epoch 63/150\n",
            "3/3 [==============================] - 0s 141ms/step - loss: 4.1544\n",
            "Epoch 64/150\n",
            "3/3 [==============================] - 0s 133ms/step - loss: 4.1539\n",
            "Epoch 65/150\n",
            "3/3 [==============================] - 0s 140ms/step - loss: 4.1265\n",
            "Epoch 66/150\n",
            "3/3 [==============================] - 0s 137ms/step - loss: 4.1008\n",
            "Epoch 67/150\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 4.0882\n",
            "Epoch 68/150\n",
            "3/3 [==============================] - 0s 134ms/step - loss: 4.0808\n",
            "Epoch 69/150\n",
            "3/3 [==============================] - 0s 140ms/step - loss: 4.0517\n",
            "Epoch 70/150\n",
            "3/3 [==============================] - 0s 139ms/step - loss: 4.0559\n",
            "Epoch 71/150\n",
            "3/3 [==============================] - 0s 140ms/step - loss: 4.0322\n",
            "Epoch 72/150\n",
            "3/3 [==============================] - 1s 222ms/step - loss: 4.0104\n",
            "Epoch 73/150\n",
            "3/3 [==============================] - 1s 249ms/step - loss: 4.0104\n",
            "Epoch 74/150\n",
            "3/3 [==============================] - 1s 254ms/step - loss: 3.9741\n",
            "Epoch 75/150\n",
            "3/3 [==============================] - 1s 239ms/step - loss: 3.9688\n",
            "Epoch 76/150\n",
            "3/3 [==============================] - 1s 142ms/step - loss: 3.9266\n",
            "Epoch 77/150\n",
            "3/3 [==============================] - 0s 133ms/step - loss: 3.9262\n",
            "Epoch 78/150\n",
            "3/3 [==============================] - 0s 132ms/step - loss: 3.8918\n",
            "Epoch 79/150\n",
            "3/3 [==============================] - 0s 138ms/step - loss: 3.9080\n",
            "Epoch 80/150\n",
            "3/3 [==============================] - 0s 135ms/step - loss: 3.8672\n",
            "Epoch 81/150\n",
            "3/3 [==============================] - 0s 136ms/step - loss: 3.8376\n",
            "Epoch 82/150\n",
            "3/3 [==============================] - 0s 143ms/step - loss: 3.8575\n",
            "Epoch 83/150\n",
            "3/3 [==============================] - 0s 137ms/step - loss: 3.8044\n",
            "Epoch 84/150\n",
            "3/3 [==============================] - 0s 145ms/step - loss: 3.8009\n",
            "Epoch 85/150\n",
            "3/3 [==============================] - 0s 140ms/step - loss: 3.7908\n",
            "Epoch 86/150\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 3.7500\n",
            "Epoch 87/150\n",
            "3/3 [==============================] - 0s 141ms/step - loss: 3.7402\n",
            "Epoch 88/150\n",
            "3/3 [==============================] - 0s 136ms/step - loss: 3.7152\n",
            "Epoch 89/150\n",
            "3/3 [==============================] - 0s 137ms/step - loss: 3.7112\n",
            "Epoch 90/150\n",
            "3/3 [==============================] - 0s 143ms/step - loss: 3.6842\n",
            "Epoch 91/150\n",
            "3/3 [==============================] - 0s 141ms/step - loss: 3.6693\n",
            "Epoch 92/150\n",
            "3/3 [==============================] - 0s 134ms/step - loss: 3.6336\n",
            "Epoch 93/150\n",
            "3/3 [==============================] - 0s 134ms/step - loss: 3.6533\n",
            "Epoch 94/150\n",
            "3/3 [==============================] - 0s 145ms/step - loss: 3.6121\n",
            "Epoch 95/150\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 3.5772\n",
            "Epoch 96/150\n",
            "3/3 [==============================] - 0s 135ms/step - loss: 3.5586\n",
            "Epoch 97/150\n",
            "3/3 [==============================] - 1s 211ms/step - loss: 3.5241\n",
            "Epoch 98/150\n",
            "3/3 [==============================] - 1s 250ms/step - loss: 3.5344\n",
            "Epoch 99/150\n",
            "3/3 [==============================] - 1s 247ms/step - loss: 3.5306\n",
            "Epoch 100/150\n",
            "3/3 [==============================] - 1s 250ms/step - loss: 3.4864\n",
            "Epoch 101/150\n",
            "3/3 [==============================] - 1s 166ms/step - loss: 3.4732\n",
            "Epoch 102/150\n",
            "3/3 [==============================] - 0s 141ms/step - loss: 3.4252\n",
            "Epoch 103/150\n",
            "3/3 [==============================] - 0s 140ms/step - loss: 3.4148\n",
            "Epoch 104/150\n",
            "3/3 [==============================] - 0s 139ms/step - loss: 3.4056\n",
            "Epoch 105/150\n",
            "3/3 [==============================] - 0s 142ms/step - loss: 3.3772\n",
            "Epoch 106/150\n",
            "3/3 [==============================] - 0s 135ms/step - loss: 3.3580\n",
            "Epoch 107/150\n",
            "3/3 [==============================] - 0s 136ms/step - loss: 3.3385\n",
            "Epoch 108/150\n",
            "3/3 [==============================] - 0s 141ms/step - loss: 3.3216\n",
            "Epoch 109/150\n",
            "3/3 [==============================] - 0s 142ms/step - loss: 3.3092\n",
            "Epoch 110/150\n",
            "3/3 [==============================] - 0s 131ms/step - loss: 3.2628\n",
            "Epoch 111/150\n",
            "3/3 [==============================] - 0s 138ms/step - loss: 3.2531\n",
            "Epoch 112/150\n",
            "3/3 [==============================] - 0s 138ms/step - loss: 3.2315\n",
            "Epoch 113/150\n",
            "3/3 [==============================] - 0s 143ms/step - loss: 3.2000\n",
            "Epoch 114/150\n",
            "3/3 [==============================] - 0s 145ms/step - loss: 3.2092\n",
            "Epoch 115/150\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 3.1656\n",
            "Epoch 116/150\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 3.1688\n",
            "Epoch 117/150\n",
            "3/3 [==============================] - 0s 135ms/step - loss: 3.1498\n",
            "Epoch 118/150\n",
            "3/3 [==============================] - 0s 146ms/step - loss: 3.1050\n",
            "Epoch 119/150\n",
            "3/3 [==============================] - 0s 143ms/step - loss: 3.1262\n",
            "Epoch 120/150\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 3.0991\n",
            "Epoch 121/150\n",
            "3/3 [==============================] - 0s 136ms/step - loss: 3.0568\n",
            "Epoch 122/150\n",
            "3/3 [==============================] - 1s 209ms/step - loss: 3.0296\n",
            "Epoch 123/150\n",
            "3/3 [==============================] - 1s 243ms/step - loss: 3.0394\n",
            "Epoch 124/150\n",
            "3/3 [==============================] - 1s 254ms/step - loss: 2.9964\n",
            "Epoch 125/150\n",
            "3/3 [==============================] - 1s 249ms/step - loss: 2.9730\n",
            "Epoch 126/150\n",
            "3/3 [==============================] - 1s 160ms/step - loss: 2.9743\n",
            "Epoch 127/150\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 2.9622\n",
            "Epoch 128/150\n",
            "3/3 [==============================] - 0s 141ms/step - loss: 2.9065\n",
            "Epoch 129/150\n",
            "3/3 [==============================] - 0s 141ms/step - loss: 2.8928\n",
            "Epoch 130/150\n",
            "3/3 [==============================] - 0s 143ms/step - loss: 2.8912\n",
            "Epoch 131/150\n",
            "3/3 [==============================] - 0s 133ms/step - loss: 2.8641\n",
            "Epoch 132/150\n",
            "3/3 [==============================] - 0s 144ms/step - loss: 2.8787\n",
            "Epoch 133/150\n",
            "3/3 [==============================] - 0s 137ms/step - loss: 2.8234\n",
            "Epoch 134/150\n",
            "3/3 [==============================] - 0s 138ms/step - loss: 2.7878\n",
            "Epoch 135/150\n",
            "3/3 [==============================] - 0s 144ms/step - loss: 2.8247\n",
            "Epoch 136/150\n",
            "3/3 [==============================] - 0s 143ms/step - loss: 2.7790\n",
            "Epoch 137/150\n",
            "3/3 [==============================] - 0s 145ms/step - loss: 2.7243\n",
            "Epoch 138/150\n",
            "3/3 [==============================] - 0s 142ms/step - loss: 2.6998\n",
            "Epoch 139/150\n",
            "3/3 [==============================] - 0s 144ms/step - loss: 2.7255\n",
            "Epoch 140/150\n",
            "3/3 [==============================] - 0s 142ms/step - loss: 2.6607\n",
            "Epoch 141/150\n",
            "3/3 [==============================] - 0s 145ms/step - loss: 2.6599\n",
            "Epoch 142/150\n",
            "3/3 [==============================] - 0s 140ms/step - loss: 2.6383\n",
            "Epoch 143/150\n",
            "3/3 [==============================] - 0s 146ms/step - loss: 2.6224\n",
            "Epoch 144/150\n",
            "3/3 [==============================] - 0s 141ms/step - loss: 2.5920\n",
            "Epoch 145/150\n",
            "3/3 [==============================] - 0s 139ms/step - loss: 2.6378\n",
            "Epoch 146/150\n",
            "3/3 [==============================] - 0s 135ms/step - loss: 2.5420\n",
            "Epoch 147/150\n",
            "3/3 [==============================] - 1s 263ms/step - loss: 2.5468\n",
            "Epoch 148/150\n",
            "3/3 [==============================] - 1s 253ms/step - loss: 2.5064\n",
            "Epoch 149/150\n",
            "3/3 [==============================] - 1s 252ms/step - loss: 2.4942\n",
            "Epoch 150/150\n",
            "3/3 [==============================] - 1s 239ms/step - loss: 2.4871\n"
          ]
        }
      ],
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=150 ) \n",
        "model.save( 'model.h5' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1MIy1j9aVTo"
      },
      "source": [
        "# Step 6: Defining Inference Models\n",
        "\n",
        "Encoder Inference Model: Takes questions as input and outputs LSTM states (h and c)\n",
        "\n",
        "Decoder Inference Model: Takes in 2 inputs one are the LSTM states, second are the answer input sequences. it will o/p the answers for questions which fed to the encoder model and it's state values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M90CPpNL0_u"
      },
      "outputs": [],
      "source": [
        "model.save('my_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpLowS27cn8X"
      },
      "outputs": [],
      "source": [
        "def make_inference_models():\n",
        "    \n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
        "    \n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding , initial_state=decoder_states_inputs)\n",
        "    \n",
        "    decoder_states = [state_h, state_c]\n",
        "\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    \n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwoYVsBTeYra"
      },
      "source": [
        "# Step 7: Talking with the Chatbot\n",
        "\n",
        "define a method str_to_tokens which converts str questions to Integer tokens with padding.\n",
        "\n",
        "1. First, we take a question as input and predict the state values using enc_model.\n",
        "2. We set the state values in the decoder's LSTM.\n",
        "3. Then, we generate a sequence which contains the <start> element.\n",
        "4. We input this sequence in the dec_model.\n",
        "5. We replace the <start> element with the element which was predicted by the dec_model and update the state values.\n",
        "6. We carry out the above steps iteratively till we hit the <end> tag or the maximum answer length.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA7Yx45Li3wo"
      },
      "outputs": [],
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "  \n",
        "    for word in words:\n",
        "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUr4SQDveVb0",
        "outputId": "7cd6527d-46d2-4e81-fd10-60f5ee37bdac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            " i am immortal end\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            " i can be a network of indefinitely end\n"
          ]
        }
      ],
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPNjNwxUl2DO"
      },
      "source": [
        "# Conversion to TFLite \n",
        "\n",
        "We can convert our seq2seq model to a TensorFlow Lite model so that we can use it on edge devices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        },
        "id": "Ywh_aJ-Ulxme",
        "outputId": "9744d53e-8655-4d78-bb73-d3b28a65eb95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/f9/4314143fc7b5850858a08f8011a9ef1fd85086332af253bd87861c9bb32c/tf_nightly-2.4.0.dev20200730-cp36-cp36m-manylinux2010_x86_64.whl (325.2MB)\n",
            "\u001b[K     |████████████████████████████████| 325.2MB 28kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.34.2)\n",
            "Collecting tb-nightly<3.0.0a0,>=2.4.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/32/6e061abc3227d12203434bb696a825c69e170d883d06ee541b1cddc3cb8a/tb_nightly-2.4.0a20200729-py3-none-any.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.30.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Collecting flatbuffers>=1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/f7/bf4c38ce1a8dc8cfd1937644f86e3a6801ca9e5a47bc47ed2d876a031285/tf_estimator_nightly-2.4.0.dev2020073001-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 41.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.12.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.9.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (49.1.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.1.0)\n",
            "Installing collected packages: tb-nightly, flatbuffers, tf-estimator-nightly, tf-nightly\n",
            "Successfully installed flatbuffers-1.12 tb-nightly-2.4.0a20200729 tf-estimator-nightly-2.4.0.dev2020073001 tf-nightly-2.4.0.dev20200730\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install tf-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3C3SlT-mboI"
      },
      "outputs": [],
      "source": [
        "# converter = tf.lite.TFLiteConverter.from_keras_model( enc_model )\n",
        "# buffer = converter.convert()\n",
        "# open( 'enc_model.tflite' , 'wb' ).write( buffer )\n",
        "\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model( dec_model )\n",
        "# open( 'dec_model.tflite' , 'wb' ).write( buffer )\n",
        "!pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7oqK_d8TRNw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}